{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo di Discesa del Gradiente\n",
    "Il metodo discusso alla fine dell'introduzione quando svolto con metodo di punto fisso e $\\phi(x) = 1$, rappresenta un'esempio di una più generica categoria di metodi noti come **metodi di ricerca in linea**.\n",
    "\n",
    "Questi algoritmi si basano sulle condizioni del primo ordine per calcolare soluzioni al problema di ottimizzazione svincolato\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} f(x),\n",
    "$$\n",
    "\n",
    "dove la funzione obiettivo $f(x)$ è derivabile almeno una volta.\n",
    "\n",
    "L'idea di questo metodo è quello di calcolare il punto di minimo $x^*$ di $f(x)$ attraverso un procedimento iterativo. In particolare, scelto un termine $x_0 \\in \\mathbb{R}^n$, si considera l'iterazione:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k p_k,\n",
    "$$\n",
    "\n",
    "dove $\\alpha_k > 0$ è detto **step-size** o **learning rate**, mentre $p_k \\in \\mathbb{R}^n$ è un vettore detto **direzione di discesa**, e viene scelto in modo tale da assicurare che $f(x_{k+1}) \\leq f(x_k)$, ovvero che l'algoritmo si sta muovendo verso la direzione del minimo **per ogni** iterato $k \\in \\mathbb{N}$. \n",
    "\n",
    "In pratica, i metodi di linea definiscono una successione di punti $\\{ x_k \\}_{k \\in \\mathbb{N}}$ tali che, per ogni $k \\in \\mathbb{N}$, $x_{k+1}$ **riduce** il valore della funzione $f(x)$, affinché se si definisce:\n",
    "\n",
    "$$\n",
    "x^* = \\lim_{k \\to \\infty} x_k,\n",
    "$$\n",
    "\n",
    "allora $\\nabla f(x^*) = 0$, ovvero il punto limite è un punto stazionario. Inoltre, se la direzione di discesa $p_k$ e il passo $\\alpha_k$ sono scelti correttamente, allora $x^*$ sarà necessariamente o un punto di minimo o un punto di sella.\n",
    "\n",
    "```{nota}\n",
    "In generale, se $f(x)$ non è convessa, non è possibile determinare a priori se il punto di convergenza di tale procedimento sia un punto di minimo globale o locale, ma solamente che $x^*$ è un punto stazionario.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo di discesa del gradiente (GD)\n",
    "Diamo una definizione:\n",
    "\n",
    "> $p_k$ è una **direzione di discesa** se esiste $\\alpha_k > 0$ tale che $f(x_k + \\alpha_k p_k) \\leq f(x_k)$.\n",
    "\n",
    "Quindi, una direzione di discesa è una direzione lungo la quale è sempre possibile procedere ad una diminuzione della funzione obiettivo, con uno step size adeguato.\n",
    "\n",
    "Chiaramente, questa definizione è poco operativa, e non è chiaro come sia possibile trovare una direzione di discesa per $f(x)$. Un risultato più operativo è il seguente:\n",
    "\n",
    "**Teorema:** Un vettore $p_k \\in \\mathbb{R}^n$ è una direzione di discesca per $f(x)$ in $x_k$ se:\n",
    "\n",
    "$$\n",
    "p_k^T \\nabla f(x_k) \\leq 0.\n",
    "$$\n",
    "\n",
    "La disequazione sopra ha una soluzione semplice. Infatti, se scegliamo $p_k = - \\nabla f(x_k)$, allora:\n",
    "\n",
    "$$\n",
    "p_k^T \\nabla f(x_k) = - (\\nabla f(x_k))^T \\nabla f(x_k) = - || \\nabla f(x_k) ||_2^2 \\leq 0.\n",
    "$$\n",
    "\n",
    "Quindi, scegliendo come direzione di discesa la direzione dell'antigradiente, si ottiene un metodo di ricerca di linea per l'ottimizzazione della funzione $f(x)$ (il più semplice possibile), definito da:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_0 \\in \\mathbb{R}^n, \\\\\n",
    "x_{k+1} = x_k - \\alpha_k \\nabla f(x_k).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Questo metodo prende il nome di **metodo di ripida discesa** o, più genericamente, **metodo di discesa del gradiente**.\n",
    "\n",
    "Un'osservazione chiave sul metodo di più ripida discesa. Si osservi che la direzione di discesa dell'antigradiente ha una semplice interpretazione grafica: ricordiamo che in generale, il gradiente $\\nabla f(x)$ di una qualunque funzione $f(x)$, rappresenta il vettore che, applicato ad $x$, punta nella direzione di massima crescita $f(x)$. Di conseguenza, ha totalmente senso che il vettore a lui opposto punti verso una direzione verso cui $f(x)$ decresce. Per questo motivo il metodo è spesso chiamato, come detto, metodo di ripida discesa. \n",
    "\n",
    "```{warning}\n",
    "Il fatto che la scelta $p_k = - \\nabla f(x_k)$ sia una direzione di discesa **NON** implica che il termine $x_k - \\alpha_k \\nabla f(x_k)$ sia necessariamente un \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
